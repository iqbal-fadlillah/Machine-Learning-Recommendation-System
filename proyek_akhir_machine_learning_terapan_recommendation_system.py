# -*- coding: utf-8 -*-
"""Proyek_Akhir_Machine_Learning_Terapan_Recommendation_System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zz-xyKgehxr1L9eH--EdkbsuxR5KoVf9

Proyek Akhir Machine Learning Terapan: Recommendation System

Nama: Muhammad Iqbal Fadlillah

Domisili: Kota Bandung, Jawa Barat

Email: m011x0093@dicoding.org, muhammadmif23@gmail.com

Terdapat dua metode yang digunakan pada Recommendation System. Metode pertama adalah Content Based Filtering. Tahap pertama yaitu mendeklarasikan library yang akan digunakan untuk membaca dataset dalam format .csv, melakukan visualisasi data, dan menghilangkan kolom yang tidak dipakai pada data set.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

"""Kemudian dilanjutkan ke tahap Data Loading upload dimana dataset akan diupload menggunakan path dari google drive. Dataset merupakan data recommendation system books recomender. Dataset tersebut diambil dari platform Kaggle dengan link berikut: https://www.kaggle.com/code/methoomirza/books-recomender-popular-collaborative-with-webapp/data Dataset terdiri dari dua file yaitu Books.csv dan Ratings.csv

Setelah diupload dataset tersebut dibaca menggunakan library pandas dan disimpan pada variabel book dan rating. Kemudian variabel data dipanggil untuk menampilkan isi Tabel pada dataset tersebut. Dapat dilihat dataset book terdiri dari 271360 baris dan 5 kolom dan dataset rating terdiri dari 1048575 baris dan 3 kolom.  
"""

book = pd.read_csv('/content/drive/MyDrive/Recomendations/Books.csv')
rating = pd.read_csv('/content/drive/MyDrive/Recomendations/Ratings.csv')

print("Books Shape: ", book.shape)
print("Ratings Shape: ", rating.shape)

"""Selanjutnya masuk ke tahap Univariate Exploratory Data Analysis dengan cara mengeksplorasi variabel book dengan menggunakan fungsi .info() dapat dilihat variabel book memiliki 5 kolom dengan tipe data object yang memiliki 271360 entri."""

book.info()

"""Kemudian masuk ke tahap data pre-processing dengan melakukan pengecekan apakah terdapat missing value pada variabel book dengan memanggil fungsi .isnull().sum() dan terdapat 1 missing value pada Book_Author dan 2 missing value pada Publisher."""

book.isnull().sum()

"""Missing value yang terdapat pada variabel book dihilangkan dengan memanggil fungs.dropna() dan disimpan pada variabel book_clean. Setelah itu dicek kembali apakah variabel book_clean memiliki missing value atau tidak dan didapati sudah tidak ada missing value. """

book_clean = book.dropna()
book_clean.isnull().sum()

"""Selanjutnya melakukan plot dari variabel book_clean sebanyak 5 data awal untuk mengetahui kolom apa saja yang akan digunakan pada variabel book_clean seperti tabel di bawah ini."""

book_clean.head()

"""Setelah mengetahui kolom apa saja yang akan digunakan, langkah selanjutnya adalah melakukan drop pada kolom yang tidak digunakan yaitu kolom 'Year_Of_Publication'dan 'Book_Author'."""

book = book_clean.drop(columns=['Year_Of_Publication', 'Book_Author'])
book

"""Hal yang sama juga dilakukan pada variabel rating, diawali dengan tahap Univariate Exploratory Data Analysis dengan cara mengeksplorasi variabel rating dengan menggunakan fungsi .info() dapat dilihat variabel book memiliki 2 kolom dengan tipe data int dan 1 kolom object yang memiliki 1048575 entri."""

rating.info()
print('Jumlah UserID: ', len(rating.UserID.unique()))

"""Sama seperti sebelumnya, masuk ke tahap data pre-processing dengan melakukan pengecekan apakah terdapat missing value pada variabel rating dengan memanggil fungsi .isnull().sum() dan tidak terdapat missing value disetiap kolomnya."""

rating.isnull().sum()

"""Dilanjutkan dengan melakukan plot dari variabel rating sebanyak 5 data awal untuk mengetahui kolom apa saja yang akan digunakan pada variabel rating seperti tabel di bawah ini."""

rating.head()

"""Masih dalam tahap pre-processing, selanjutnya adalah menyatukan variabel rating dan book_clean menjadi satu dataframe yang sama menggunakan fungsi pd.merge dari library pandas yang disimpan pada variabel all_book. Dapat dilihat terdapat 1048575 baris dan 5 kolom pada variabel all_book"""

all_book = pd.merge(rating, book_clean[['ISBN','Book_Title','Publisher']], on='ISBN', how='left')
all_book

"""Selanjutnya masuk ke dalam tahap data preparation, pada tahap ini variabel book_clean dan rating yang sudah disatukan disimpan menjadi variabel all_book. Setelah disatukan perlu dicek kembali apakah terdapat missing value dari variabel tersebut dengan cara yang sama seperti sebelumnya. Dapat dilihat terdapat missing value di atribut Book_Title sebanyak 107466 dan Publisher sebanyak 107466."""

all_book.isnull().sum()

"""Sama seperti sebelumnya, untuk menghilangkan missing value menggunakan fungsi .dropna() dan kembali mengecek variabel all_book apakah masih terdapat missing value atau tidak. Berdasarkan hasil dibawah ini dapat dilihat sudah tidak ada missing value pada variabel all_book."""

all_book = all_book.dropna()
all_book.isnull().sum()

"""Dilakukan pendeklarasian variabel all_book untuk mengetahui baris dan kolom yang terdapat pada variabel tersebut."""

all_book

"""Karena pada sistem rekomendasi yang akan dibuat akan menghasilkan rekomendasi buku berdasarkan judul bukunya maka perlu diketahui banyaknya judul buku yang terdapat pada dataset yaitu dengan cara mendeklarasikan fungsi len(). Pada fungsi len dimasukan parameter all_book.Book_Title.unique() bertujuan memanggil atribut Book_Title pada variabel all_book."""

print('Banyak judul buku: ', len(all_book.Book_Title.unique()))

"""Masih di tahap data preparation, pertama variabel all_book akan disimpan pada variabel preparation. Langkah selanjutnya adalah menghilangkan data ganda pada variabel preparation berdasarkan atribut UserID dengan mendeklarasikan fungsi .drop_duplicates(). Diperoleh jumlah baris sebanyak 83643 yang semula sebanyak 941109."""

preparation = all_book
preparation = preparation.drop_duplicates('UserID')
preparation

"""Tahap selanjutnya adalah melakukan sorting pada atribut UserID secara ascending agar nilai atribut USerID akan berurut dari nilai terkecil hingga terbesar dan disimpan pada variabel fix_book."""

fix_book = preparation.sort_values('UserID', ascending=True)
fix_book

"""Kemudian perlu dilakukan konversi data series menjadi list pada atribut UserID, Book_Rating, ISBN, Book_Title, dan Publisher. Dalam hal ini, menggunakan fungsi tolist() dari library numpy. Selanjutnya mengecek apakah jumlah data dari setiap list sudah sama banyak dengan cara mendeklarasikan fungsi len() dari setiap variabel."""

userID = preparation['UserID'].tolist()

book_rating = preparation['Book_Rating'].tolist()

isbn = preparation['ISBN'].tolist()

book_title = preparation['Book_Title'].tolist()

book_publisher = preparation['Publisher'].tolist()
 
print(len(userID))
print(len(book_rating))
print(len(isbn))
print(len(book_title))
print(len(book_publisher))

"""Masih di tahap data preparation tahap berikutnya adalah membuat dictionary untuk menentukan pasangan key-value pada data userID,isbn, book_rating, book_title, dan book_publisher yang telah disiapkan sebelumnya ke dalam variabel book_rate."""

book_rate = pd.DataFrame({
    'id': userID,
    'isbn': isbn,
    'book_rating': book_rating,
    'book_title': book_title,
    'book_publisher': book_publisher
})
book_rate

"""Tahap berikutnya adalah melakukan drop pada variabel book_rate dikarenakan jumlah dataset yang terlalu besar sebanyak 83643 baris. Variabel book_rate akan di drop pada range (500, 83643) sehingga dataset yang akan digunakan sebanyak 500 sampel data dan disimpan pada variabel book_new. """

book_new = book_rate.drop(labels=range(500, 83643), axis=0)
book_new

"""Selanjutnya dilakukan eksplorasi variabel book_new dengan menggunakan fungsi .info() dapat dilihat variabel book memiliki 5 kolom dengan 3 tipe data object dan 2 int yang memiliki 500 entri."""

book_new.info()

"""Selanjutnya book_new akan disimpan pada variabel baru yaitu data dan dicek apakah variabel book_new sudah memiliki data yang sesuai yang nantinya akan digunakan sebagai dataset.  """

data = book_new
data.sample(5)

"""Pada tahap ini, merupakan inti dari model sistem rekomendasi yaitu penentuan metode yang akan digunakan. Pada proyek ini akan digunakan dua metode yaitu Content Based Filtering dan Collaborative Filtering. Teknik pertama yang akan digunakan adalah Content Based Filtering.

Pada metode Content Based Filtering akan dibuat model sistem rekomendasi sederhana berdasarkan publisher yang dimiliki dari sebuah buku. 

Tahap selanjutnya adalah menggunakan teknik TF-IDF Vectorizer. Teknik tersebut akan digunakan pada sistem rekomendasi untuk menemukan representasi fitur penting dari setiap kategori buku. Skor dalam TF-IDF digunakan untuk mengamati istilah-istilah berbeda yang mengandung informasi penting dalam dokumen tertentu. Teknik ini menggunakan fungsi tfidfvectorizer() dari library sklearn.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf = TfidfVectorizer()

tf.fit(data['book_publisher']) 

tf.get_feature_names()

"""Kemudian merubah bentuk fit dan transformasi ke dalam bentuk matriks, dapat dilihat ukuran matriks yang diperoleh adalah 500 x 293. Nilai 500 merupakan ukuran dataset dan 293 merupakan matrik dari atribut book_publisher. """

tfidf_matrix = tf.fit_transform(data['book_publisher']) 

tfidf_matrix.shape

"""Untuk menghasilkan vektor tf-idf dalam bentuk matriks dapat menggunakan fungsi todense(). Objek matriks dengan bentuk yang sama dan berisi data yang sama yang diwakili oleh matriks sparse, dengan urutan memori yang diminta."""

tfidf_matrix.todense()

"""Tahap selanjutnya adalah membuat dataframe untuk melihat matriks tf-idf yang telah dibuat pada proses sebelumnya. Kolom diisi dengan publisher buku dan bari diisi dengan judul bukui. Data frame yang dihasilkan berjumlah 20 sampel acak judul buku, dan 10 sampel acak publisher."""

pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names(),
    index=data.book_title
).sample(20, axis=1).sample(10, axis=0)

"""Selanjutnya masuk ke tahap cosine similarity, pada tahap ini judul buku yang akan dihasilkan akan dihitung similaritynya berdasarkan matriks tfidf yang sudah dihasilkan sebelumnya menggunakan fungsi cosine_similarity(tf-idf matrix)."""

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

"""Sama seperti pada tf-idf, pada cosine similarity juga dibuat dataframe yang berisikan matriks hasil cosine similarity untuk membandingkan kemiripan antara judul satu buku dengan buku lainnya. 

Bentuk matriks cosine similarity adalah (500, 500) menandakan seluruh dataset sudah diidentifikasi menggunakan cosine similarity. Selanjutnya, ditampilkan sampel acak sebanyak 10 sampel pada baris dan kolom. Banyak judul buku yang tidak sesuai karena nilai yang diperoleh sebesar 0 dikarenakan data sampel diambil secara acak dan menyebabkan masih banyak judul buku yang tidak muncul.
"""

cosine_sim_df = pd.DataFrame(cosine_sim, index=data['book_title'], columns=data['book_title'])
print('Shape:', cosine_sim_df.shape)
 
cosine_sim_df.sample(10, axis=1).sample(10, axis=0)

"""Tahap selanjutnya adalah membuat fungsi book_recommendations dengan beberapa parameter sebagai berikut:


nama_buku : nama buku berdasarkan index kemiripan dataframe.

similarity_data : Dataframe mengenai similarity yang telah diidentifikasi sebelumnya yaitu cosine_sim_df.

items : nama dan fitur yang digunakan untuk mendefinisikan kemiripan, dalam hal ini adalah book_title dan book_publisher.

k : Banyak rekomendasi yang ingin diberikan.


Fungsi book_recommendations akan diambil k jumlah dengan nilai similarity terbesar pada index matrix yang diberikan (i).
"""

def book_recommendations(nama_buku, similarity_data=cosine_sim_df, items=data[['book_title', 'book_publisher']], k=5):
 
    index = similarity_data.loc[:,nama_buku].to_numpy().argpartition(
        range(-1, -k, -1))
    
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    
    closest = closest.drop(nama_buku, errors='ignore')
 
    return pd.DataFrame(closest).merge(items).head(k)

"""Tahap selanjutnya adalah mengambil satu sampel buku yang berada pada dataset dalam kasus ini berada di dalam variabel data. Saya mengambil contoh untuk memasukan judul buku The Sigma Protocol."""

data[data.book_title.eq('The Sigma Protocol')]

"""Selanjutnya adalah menguji coba apakah sistem rekomendasi sudah berjalan dengan baik dengan cara memanggil fungsi book_recommendations dan mengisi parameter dengan judul buku yang ingin dicari similaritynya berdasarkan publisher buku tersebut. Diperoleh 5 judul buku yang memiliki kemiripin yang sama dengan buku The Sigma Protocol berdasarkan book_publishernya.

Maka sistem rekomendasi menggunakan metode Content Based Filtering sudah berhasil dibuat dengan menghasilkan output berupa Top-N Recommendation.
"""

book_recommendations('The Sigma Protocol')

"""Metode kedua adalah Collaborative Filtering yaitu membuat sistem rekomendasi berdasarkan pengalaman user yang sudah membaca buku tertentu. Kemudian sistem akan merekomendasikan buku yang belum pernah dibaca oleh user. Langkah pertama yang dilakukan adalah mendeklarasikan library yang akan digunakan."""

import pandas as pd
import numpy as np 
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

"""Dataset yang digunakan masih sama seperti sebelumnya yaitu variabel book_new yang sekarang disimpan pada variabel df agar tidak menimpa variabel sebelumnya."""

df = book_new
df

"""Selanjutnya adalah tahap data preparation. Pada tahap ini, fitur ‘id’ pada data akan disandikan (encode) ke dalam indeks integer. hasil dari encode adalah dengan hasil seperti berikut."""

user_ids = df['id'].unique().tolist()
print('list userID: ', user_ids)

user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

"""Sama halnya dengan fitur ‘id’, fitur 'isbn' pada data juga akan disandikan (encode) ke dalam indeks integer. hasil dari encode adalah dengan hasil seperti berikut."""

book_ids = df['isbn'].unique().tolist()

book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}

book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}

"""Tahap berikutnya adalah memetakan (mapping) id yang sudah di encode ke dalam dataframe user dan isbn ke dataframe book."""

df['user'] = df['id'].map(user_to_user_encoded)
 
df['book'] = df['isbn'].map(book_to_book_encoded)

"""Selanjutnya cek beberapa hal dalam data seperti jumlah user, jumlah buku, dan mengubah nilai rating menjadi float serta menghitung nilai min max dari atribut book_rating kemudian print hasilnya."""

num_users = len(user_to_user_encoded)
print(num_users)
 
num_book = len(book_encoded_to_book)
print(num_book)
 
df['rating'] = df['book_rating'].values.astype(np.float32)
 
min_rating = min(df['book_rating'])
 
max_rating = max(df['book_rating'])
 
print('Number of User: {}, Number of Book: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating
))

"""Tahap berikutnya adalah mengacak dataset sebelum dilakukan pembagian dataset  data training dan validasi. Dataset diacak agar sebaran data lebih random, digunakan parameter randomstate dengan nilai 42 dan parameter frac merupakan nilai float."""

df = df.sample(frac=1, random_state=42)
df

"""Setelah dataset diacak dilanjutkan membagi dataset menjadi data train dan validasi dengan komposisi 90:10. Namun sebelum membagi, perlu ada pemetaan (mapping) data user dan book menjadi satu value yang sama dan juga membuat rating dalam skala 0 sampai 1 agar mudah dalam melakukan proses training. """

x = df[['user', 'book']].values
 
y = df['book_rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.9 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

"""Tahap selanjutnya adalah proses training, sebelum melakukan proses training dibuat class RecommenderNet dengan library keras. Model akan menghitung skor kecocokan antara user dan book dengan teknik embedding. Pada fungsi tersebut dilakukan proses embedding terhadap data user dan book. Setelah itu dilakukan operasi dot product antara user_vector dan book_vector, user_vector dan book_vector berisikan embedding dari user dan book itu sendiri. Nilai kecocokan yang diperoleh akan memiliki nilai dalam skala [0,1] dengan fungsi aktivasi sigmoid."""

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_book, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_book = num_book
        self.embedding_size = embedding_size
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-3),
        )
        self.user_bias = layers.Embedding(num_users, 1)
        self.book_embedding = layers.Embedding(
            num_book,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-3),
        )
        self.book_bias = layers.Embedding(num_book, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        book_vector = self.book_embedding(inputs[:, 1])
        book_bias = self.book_bias(inputs[:, 1])
        dot_user_book = tf.tensordot(user_vector, book_vector, 2)
        x = dot_user_book + user_bias + book_bias
        return tf.nn.sigmoid(x)

"""Selanjutnya melakukan proses compile pada model, model berisikan fungsi RecommenderNet yang sudah dideklarasikan sebelumnya. model.compile memiliki beberapa nilai parameter yaitu Binary Crossentropy untuk menghitung nilai loss function, kemudian Adam Adaptive Moment Estimation sebagai parameter optimizer dengan learning rate sebesar 0.0001, dan root mean squared error (RMSE) sebagai metrics evaluation yang digunakan pada model tersebut. """

model = RecommenderNet(num_users, num_book, 50) 
 
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    run_eagerly= True,
    optimizer = keras.optimizers.Adam(learning_rate=0.0001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Selanjutnya dilakukan training pada model dengan memanggil fungsi model.fit() yang didalamnya dideklarasikan beberapa parameter seperti data latih dari atribut dan label, batch_size yang digunakan sebesar 8, epoch sebesar 25, dan vaildation data pada variabel x_val y_val."""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 25,
    validation_data = (x_val, y_val)
)

"""Selanjutnya memplot hasil training model ke dalam bentuk grafik dengan sumbu y sebagai nilai loss dan sumbu x sebagai banyaknya epoch."""

plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("model loss")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show()

"""Tahap selanjutnya untuk mendapatkan sistem rekomendasi buku adalah mendeklarasikan sampel user_id secara acak (dalam kasus ini saya mengambil sampel dari user id 276744) kemudian mendefinisikan variabel book_read_by_user yang merupakan daftar book yang sudah pernah dibaca oleh user tersebut. Kemudian mendeklarasikan variabel book_not_read merupakan daftar buku yang belum pernah dibaca oleh user tersebut. Variabel book_not_read nantinya akan menjadi acuan dari buku  yang direkomendasikan. """

book_df = book_new
df = pd.read_csv('/content/drive/MyDrive/Recomendations/Ratings.csv')
 
user_id = 276744
book_read_by_user = df[df.UserID == user_id]

book_not_read = book_df[~book_df['id'].isin(book_read_by_user.ISBN.values)]['isbn'] 
book_not_read = list(
    set(book_not_read)
    .intersection(set(book_to_book_encoded.keys()))
)
 
book_not_read = [[book_to_book_encoded.get(x)] for x in book_not_read]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_read), book_not_read)
)

"""Tahap terakhir untuk memperoleh hasil rekomendasi buku dengan N-Top recommendation seperti contoh pada modul adalah dengan cara mendeklarasikan model.predict() dimana model merupakan hasil training sebelumnya. Hasil dari prediksi akan menampilkan 10 jenis buku yang sesuai dengan user_id yang sudah dideklarasikan sebelumnya. Buku yang ditampilkan berdasarkan perkiraan rating tertinggi yang akan diberikan user_id terhadap buku yang belum pernah dibaca dikarenakan metode ini merupakan metode Collaborative Filtering dengan hasil seperti dibawah. """

ratings = model.predict(user_book_array).flatten()
 
print('----' * 8)
print('Top 10 books recommendation')
print('----' * 8)
book_recommendation = pd.DataFrame({
    'isbn': book_ids,
    'ratings': ratings
})
final_book_recommendation = book_recommendation.sort_values(by='ratings', ascending=False)
final_book_recommendation = final_book_recommendation.merge(book_new, left_on='isbn', right_on='isbn')
book_recommendation_clean = final_book_recommendation.drop(columns=['book_rating','user', 'book', 'rating', 'book_publisher'])
book_recommendation_clean.head(10)